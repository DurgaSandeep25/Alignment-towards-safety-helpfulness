import os
import gc
import torch
import tqdm as notebook_tqdm
from tqdm import tqdm
import argparse
import json

import transformers
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, BitsAndBytesConfig
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
# from peft import prepare_model_for_int8_training
from trl import DPOTrainer, SFTTrainer
import bitsandbytes as bnb
from trl import DataCollatorForCompletionOnlyLM

from datasets import load_dataset, Dataset, load_from_disk
from transformers import TrainerCallback, TrainerState, TrainerControl
from transformers.trainer_utils impbort PREFIX_CHECKPOINT_DIR

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

os.environ["TOKENIZERS_PARALLELISM"] = "false" # to avoid warning "Tokenizer deadlocks"
os.environ["HF_TOKEN"] = "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" # my huggingface key to access llama models



def do_required(model_path, resps_path, save_path):
    # loading the model and tokenizer
    model = AutoModelForSequenceClassification.from_pretrained(model_path, 
                                                quantization_config=bnb_config, 
                                                device_map={"": 0},
                                                trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

    print(f"response path: {resps_path}")
    print(f"model_path: {model_path}")
    print(f"save_path: {save_path}")
    resps_dataset = Dataset.from_json(resps_path)
    # number of responses generated by the model for RAFT, here 8
    k = len(resps_dataset['responses'][0])
    # total number of prompts
    n_rows = len(resps_dataset)
    
    # scores for all responses given by the model
    # here list[list[float]]
    scores = []
    for j in tqdm(range(n_rows)):
        scores_ = []
        for i in range(k):
            prompt = resps_dataset['instruction'][j]
            resp = resps_dataset['responses'][j][i]
            inputs = tokenizer(prompt, resp, return_tensors='pt')
            score = model(**inputs).logits[0].cpu().detach()
            scores_.append(score.item())
            torch.cuda.empty_cache()
        scores.append(scores_)

    results = []
    for i in range(n_rows):
        results.append({
            "prompt": resps_dataset['instruction'][i],
            "responses": resps_dataset['responses'][i],
            "scores": scores[i],
            "best_score_idx": scores[i].index(max(scores[i])),
            "worst_score_idx": scores[i].index(min(scores[i])),
            # "safety": resps_dataset['safety'][i]
        })

    # saving the files
    with open(save_path, 'w') as f:
        json.dump(results, f)

if __name__=="__main__":
    resps_path = "/scratch/workspace/asureddy_umass_edu-llm_alignment/saved_models/llama-7b-500.json"
    save_path = "test.json"

    parser = argparse.ArgumentParser(description="Reward model scorer")
    
    parser.add_argument("--model_path", type=str, default="OpenAssistant/reward-model-deberta-v3-large-v2", help="Model name or path")
    parser.add_argument("--resps_path", type=str, default="/scratch/workspace/asureddy_umass_edu-llm_alignment/saved_models/llama-7b-500.json", help="responses path")
    parser.add_argument("--save_path", type=str, default="test.json", help="path to save results")

    input_args = parser.parse_args()

    do_required(input_args.model_path, input_args.resps_path, input_args.save_path)
    