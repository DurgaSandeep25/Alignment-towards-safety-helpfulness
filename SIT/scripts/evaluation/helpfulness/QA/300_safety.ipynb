{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this notebook, we will look at the performance of BoolQ, PIQA and OpenBookQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vLLM for infernce\n",
    "from datasets import load_from_disk, Dataset, load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-04 02:12:10 llm_engine.py:87] Initializing an LLM engine with config: model='/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/models/safety_llama_replicate_300_safe/merged_model', tokenizer='/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/models/safety_llama_replicate_300_safe/merged_model', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-04 02:16:02 llm_engine.py:357] # GPU blocks: 1976, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/models/safety_llama_replicate_300_safe/merged_model\"\n",
    "model = LLM(model=model_name,\n",
    "                    tokenizer=model_name, \n",
    "                    tensor_parallel_size=torch.cuda.device_count(), \n",
    "                    seed=seed,\n",
    "                    gpu_memory_utilization=0.9, \n",
    "                    dtype=torch.float16,\n",
    "                    enforce_eager=True,\n",
    "                    max_model_len=1024 # 512 is small for the BoolQ dataset, so changing it to 1024\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper functions\n",
    "def get_prompt(example, data = 'boolq'):\n",
    "    if data == 'boolq':\n",
    "        prompt = f\"### Instruction: Answer the following question (True/False) based on the passage.\\n\\n### Passage:\\n{example['passage']}\\n\\n### Question: {example['question']}\\n\\n### Answer: \"\n",
    "    elif data == 'obqa':\n",
    "        prompt = f\"### Instruction: Answer the following mulitple-choice question (A/B/C/D).\\n\\n### Question:\\n{example['question_stem']}\\n\\n### Choices: \\n{example['choices']}\\n### Answer: \"\n",
    "    elif data == 'piqa':\n",
    "        prompt = f\"### Instruction: Answer the following mulitple-choice question (A/B).\\n\\n### Question:\\n{example['goal']}\\n\\n### Choices: \\n{example['choices']}\\n\\n### Answer: \"\n",
    "    else:\n",
    "        print(\"############# Issue with the prompt\")\n",
    "        return \"\"\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_obqa_label(example):\n",
    "    res = \"\"\n",
    "    for i in range(len(example['text'])):\n",
    "        res += f\"{example['label'][i]}) {example['text'][i]}\\n\"\n",
    "    return res\n",
    "\n",
    "def preprocess_response_true_or_false(text):\n",
    "    text = text.lower()\n",
    "    if 'true' in text:\n",
    "        return True\n",
    "    if 'false' in text:\n",
    "        return False\n",
    "    return 'Other'\n",
    "\n",
    "def preprocess_response_choices(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    if ('a' in text) and ('b' not in text) and ('c' not in text) and ('d' not in text):\n",
    "        return 'A'\n",
    "    if ('a' not in text) and ('b' in text) and ('c' not in text) and ('d' not in text):\n",
    "        return 'B'\n",
    "    if ('a' not in text) and ('b' not in text) and ('c' in text) and ('d' not in text):\n",
    "        return 'C'\n",
    "    if ('a' not in text) and ('b' not in text) and ('c' not in text) and ('d' in text):\n",
    "        return 'D'\n",
    "    return 'Others'\n",
    "\n",
    "def preprocess_piqa_response(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    if ('0' in text) and ('1' not in text):\n",
    "        return 0\n",
    "    if ('1' in text) and ('0' not in text):\n",
    "        return 1\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12697, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  answer  \\\n",
       "0    do iran and afghanistan speak the same language    True   \n",
       "1  do good samaritan laws protect those who help ...    True   \n",
       "\n",
       "                                             passage  \n",
       "0  Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...  \n",
       "1  Good Samaritan laws offer legal protection to ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BoolQ dataset\n",
    "bool_q = load_dataset(\"google/boolq\")\n",
    "boolq = pd.concat([bool_q['train'].to_pandas(), bool_q['validation'].to_pandas()])\n",
    "print(boolq.shape)\n",
    "boolq.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the respone prompt\n",
    "boolq['inference_prompt'] = boolq.apply(lambda x: get_prompt(x, 'boolq'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Answer the following question (True/False) based on the passage.\n",
      "\n",
      "### Passage:\n",
      "Windows Movie Maker (formerly known as Windows Live Movie Maker in Windows 7) is a discontinued video editing software by Microsoft. It is a part of Windows Essentials software suite and offers the ability to create and edit videos as well as to publish them on OneDrive, Facebook, Vimeo, YouTube, and Flickr.\n",
      "\n",
      "### Question: is windows movie maker part of windows essentials\n",
      "\n",
      "### Answer: \n"
     ]
    }
   ],
   "source": [
    "print(boolq['inference_prompt'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq_sampling = SamplingParams(n=1, \n",
    "                                 max_tokens=3,\n",
    "                                 # top_k=40,\n",
    "                                 top_p=0.9,\n",
    "                                 temperature=0.0,\n",
    "                                 # frequency_penalty=1.0\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.52it/s]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(boolq['inference_prompt'].tolist()[0], boolq_sampling)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFalse.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 1/12697 [00:07<27:10:00,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-04 02:16:22 scheduler.py:195] Input prompt (1072 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  98%|█████████▊| 12403/12697 [10:32<00:15, 18.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-04 02:26:46 scheduler.py:195] Input prompt (1402 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 12697/12697 [10:39<00:00, 19.86it/s]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(boolq['inference_prompt'].tolist(), boolq_sampling)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq_results = []\n",
    "for ele in output:\n",
    "    boolq_results.append(ele.outputs[0].text)\n",
    "boolq['generated_response'] = boolq_results\n",
    "boolq['predicted_label'] = boolq['generated_response'].apply(preprocess_response_true_or_false)\n",
    "boolq['predicted_label'] = boolq['predicted_label'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "True     8220\n",
       "False    3776\n",
       "Other     701\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq['predicted_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "True     0.622746\n",
       "False    0.377254\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq['answer'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.65424903520517"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq[boolq['answer'].apply(str) == boolq['predicted_label']].shape[0]*100.0/boolq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq.to_csv(\"./results/boolq_300_safety.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenBookQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5457, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_stem</th>\n",
       "      <th>choices</th>\n",
       "      <th>answerKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7-980</td>\n",
       "      <td>The sun is responsible for</td>\n",
       "      <td>{'text': ['puppies learning new tricks', 'chil...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7-584</td>\n",
       "      <td>When standing miles away from Mount Rushmore</td>\n",
       "      <td>{'text': ['the mountains seem very close', 'th...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                 question_stem  \\\n",
       "0  7-980                    The sun is responsible for   \n",
       "1  7-584  When standing miles away from Mount Rushmore   \n",
       "\n",
       "                                             choices answerKey  \n",
       "0  {'text': ['puppies learning new tricks', 'chil...         D  \n",
       "1  {'text': ['the mountains seem very close', 'th...         D  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenBookQA\n",
    "ob_qa = load_dataset(\"allenai/openbookqa\")\n",
    "obqa = pd.concat([ob_qa['train'].to_pandas(), ob_qa['validation'].to_pandas()])\n",
    "print(obqa.shape)\n",
    "obqa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obqa['choices'] = obqa['choices'].apply(get_obqa_label)\n",
    "obqa['inference_prompt'] = obqa.apply(lambda x: get_prompt(x, 'obqa'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "obqa_sampling = SamplingParams(n=1, \n",
    "                               max_tokens=2,\n",
    "                               top_p=0.9,\n",
    "                               temperature=0.0,\n",
    "                               # frequency_penalty=1.0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0, len(obqa)-1)\n",
    "    output = model.generate(obqa['inference_prompt'].tolist()[index], obqa_sampling)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(output[0].outputs[0].text, obqa['answerKey'].tolist()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5457/5457 [01:52<00:00, 48.37it/s]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(obqa['inference_prompt'].tolist(), obqa_sampling)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "C         3450\n",
       "D         1074\n",
       "B          537\n",
       "A          395\n",
       "Others       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "for ele in output:\n",
    "    all_results.append(ele.outputs[0].text)\n",
    "obqa['generated_response'] = all_results\n",
    "obqa['predicted_label'] = obqa['generated_response'].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "obqa['predicted_label'] = obqa['predicted_label'].apply(preprocess_response_choices)\n",
    "obqa.predicted_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answerKey\n",
       "A    0.275609\n",
       "B    0.247389\n",
       "D    0.246839\n",
       "C    0.230163\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obqa['answerKey'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.62396921385377"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obqa[obqa['answerKey'] == obqa['predicted_label']].shape[0]*100.0/obqa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obqa.to_csv(\"./results/obqa_300_safety.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17951, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goal</th>\n",
       "      <th>sol1</th>\n",
       "      <th>sol2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When boiling butter, when it's ready, you can</td>\n",
       "      <td>Pour it onto a plate</td>\n",
       "      <td>Pour it into a jar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To permanently attach metal legs to a chair, y...</td>\n",
       "      <td>Weld the metal together to get it to stay firm...</td>\n",
       "      <td>Nail the metal together to get it to stay firm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                goal  \\\n",
       "0      When boiling butter, when it's ready, you can   \n",
       "1  To permanently attach metal legs to a chair, y...   \n",
       "\n",
       "                                                sol1  \\\n",
       "0                               Pour it onto a plate   \n",
       "1  Weld the metal together to get it to stay firm...   \n",
       "\n",
       "                                                sol2  label  \n",
       "0                                 Pour it into a jar      1  \n",
       "1  Nail the metal together to get it to stay firm...      0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PIQA dataset\n",
    "piqa_data = load_dataset(\"piqa\")\n",
    "piqa = pd.concat([piqa_data['train'].to_pandas(), piqa_data['validation'].to_pandas(), piqa_data['test'].to_pandas()])\n",
    "piqa = piqa[piqa['label'] != -1]\n",
    "print(piqa.shape)\n",
    "piqa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    8988\n",
       "0    8963\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piqa.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "piqa['choices'] = piqa.apply(lambda x: f\"0) {x['sol1']}\\n1) {x['sol2']}\", 1)\n",
    "piqa['inference_prompt'] = piqa.apply(lambda x: get_prompt(x, 'piqa'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "piqa_sampling = SamplingParams(n=1, \n",
    "                               max_tokens=2,\n",
    "                               top_p=0.9,\n",
    "                               temperature=0.0,\n",
    "                               # frequency_penalty=1.0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0, len(piqa)-1)\n",
    "    output = model.generate(piqa['inference_prompt'].tolist()[index], piqa_sampling)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(output[0].outputs[0].text, piqa['label'].tolist()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  28%|██▊       | 5095/17951 [02:48<06:58, 30.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-04 02:31:56 scheduler.py:195] Input prompt (1341 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  71%|███████   | 12763/17951 [06:46<02:39, 32.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-04 02:35:54 scheduler.py:195] Input prompt (1322 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 17951/17951 [09:22<00:00, 31.91it/s]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(piqa['inference_prompt'].tolist(), piqa_sampling)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "1        12126\n",
       "0         5808\n",
       "Other       17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "for ele in output:\n",
    "    all_results.append(ele.outputs[0].text)\n",
    "piqa['generated_response'] = all_results\n",
    "piqa['predicted_label'] = piqa['generated_response'].apply(preprocess_piqa_response)\n",
    "piqa['predicted_label'] = piqa['predicted_label'].apply(str)\n",
    "piqa.predicted_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.500696\n",
       "0    0.499304\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piqa['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.06896551724138"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piqa[piqa['label'].apply(str) == piqa['predicted_label']].shape[0]*100.0/piqa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "piqa.to_csv(\"./results/piqa_300_safety.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Token Probability ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
