{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this notebook, we will look at the performance of BoolQ, PIQA and OpenBookQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vLLM for infernce\n",
    "from datasets import load_from_disk, Dataset, load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 21:28:14 llm_engine.py:87] Initializing an LLM engine with config: model='/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/models/safety_llama_replicate/merged_model', tokenizer='/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/models/safety_llama_replicate/merged_model', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 21:28:31 llm_engine.py:357] # GPU blocks: 137, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/models/safety_llama_replicate/merged_model\"\n",
    "model = LLM(model=model_name,\n",
    "                    tokenizer=model_name, \n",
    "                    tensor_parallel_size=torch.cuda.device_count(), \n",
    "                    seed=seed,\n",
    "                    gpu_memory_utilization=0.9, \n",
    "                    dtype=torch.float16,\n",
    "                    enforce_eager=True,\n",
    "                    max_model_len=1024 # 512 is small for the BoolQ dataset, so changing it to 1024\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper functions\n",
    "def get_prompt(example, data = 'boolq'):\n",
    "    if data == 'boolq':\n",
    "        prompt = f\"### Instruction: Answer the following question (True/False) based on the passage.\\n\\n### Passage:\\n{example['passage']}\\n\\n### Question: {example['question']}\\n\\n### Answer: \"\n",
    "    elif data == 'obqa':\n",
    "        prompt = f\"### Instruction: Answer the following mulitple-choice question (A/B/C/D).\\n\\n### Question:\\n{example['question_stem']}\\n\\n### Choices: \\n{example['choices']}\\n### Answer: \"\n",
    "    elif data == 'piqa':\n",
    "        prompt = f\"### Instruction: Answer the following mulitple-choice question (A/B).\\n\\n### Question:\\n{example['goal']}\\n\\n### Choices: \\n{example['choices']}\\n\\n### Answer: \"\n",
    "    else:\n",
    "        print(\"############# Issue with the prompt\")\n",
    "        return \"\"\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_obqa_label(example):\n",
    "    res = \"\"\n",
    "    for i in range(len(example['text'])):\n",
    "        res += f\"{example['label'][i]}) {example['text'][i]}\\n\"\n",
    "    return res\n",
    "\n",
    "def preprocess_response_true_or_false(text):\n",
    "    text = text.lower()\n",
    "    if 'true' in text:\n",
    "        return True\n",
    "    if 'false' in text:\n",
    "        return False\n",
    "    return 'Other'\n",
    "\n",
    "def preprocess_response_choices(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    if ('a' in text) and ('b' not in text) and ('c' not in text) and ('d' not in text):\n",
    "        return 'A'\n",
    "    if ('a' not in text) and ('b' in text) and ('c' not in text) and ('d' not in text):\n",
    "        return 'B'\n",
    "    if ('a' not in text) and ('b' not in text) and ('c' in text) and ('d' not in text):\n",
    "        return 'C'\n",
    "    if ('a' not in text) and ('b' not in text) and ('c' not in text) and ('d' in text):\n",
    "        return 'D'\n",
    "    return 'Others'\n",
    "\n",
    "def preprocess_piqa_response(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    if ('a' in text) and ('b' not in text):\n",
    "        return 0\n",
    "    if ('b' in text) and ('a' not in text):\n",
    "        return 1\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12697, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  answer  \\\n",
       "0    do iran and afghanistan speak the same language    True   \n",
       "1  do good samaritan laws protect those who help ...    True   \n",
       "\n",
       "                                             passage  \n",
       "0  Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...  \n",
       "1  Good Samaritan laws offer legal protection to ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BoolQ dataset\n",
    "bool_q = load_dataset(\"google/boolq\")\n",
    "boolq = pd.concat([bool_q['train'].to_pandas(), bool_q['validation'].to_pandas()])\n",
    "print(boolq.shape)\n",
    "boolq.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the respone prompt\n",
    "boolq['inference_prompt'] = boolq.apply(lambda x: get_prompt(x, 'boolq'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Answer the following question (True/False) based on the passage.\n",
      "\n",
      "### Passage:\n",
      "Windows Movie Maker (formerly known as Windows Live Movie Maker in Windows 7) is a discontinued video editing software by Microsoft. It is a part of Windows Essentials software suite and offers the ability to create and edit videos as well as to publish them on OneDrive, Facebook, Vimeo, YouTube, and Flickr.\n",
      "\n",
      "### Question: is windows movie maker part of windows essentials\n",
      "\n",
      "### Answer: \n"
     ]
    }
   ],
   "source": [
    "print(boolq['inference_prompt'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq_sampling = SamplingParams(n=1, \n",
    "                                 max_tokens=3,\n",
    "                                 # top_k=40,\n",
    "                                 top_p=0.9,\n",
    "                                 temperature=0.0,\n",
    "                                 # frequency_penalty=1.0\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.54it/s]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(boolq['inference_prompt'].tolist()[0], boolq_sampling)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFalse.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   2%|▏         | 193/12697 [00:10<11:29, 18.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-03 20:12:53 scheduler.py:195] Input prompt (1072 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  99%|█████████▉| 12561/12697 [11:55<00:07, 17.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-03 20:24:38 scheduler.py:195] Input prompt (1402 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 12697/12697 [12:02<00:00, 17.59it/s]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(boolq['inference_prompt'].tolist(), boolq_sampling)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq_results = []\n",
    "for ele in output:\n",
    "    boolq_results.append(ele.outputs[0].text)\n",
    "boolq['generated_response'] = boolq_results\n",
    "boolq['predicted_label'] = boolq['generated_response'].apply(preprocess_response_true_or_false)\n",
    "boolq['predicted_label'] = boolq['predicted_label'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "True     8520\n",
       "False    3584\n",
       "Other     593\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq['predicted_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "True     0.622746\n",
       "False    0.377254\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq['answer'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.08765850200835"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq[boolq['answer'].apply(str) == boolq['predicted_label']].shape[0]*100.0/boolq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq.to_csv(\"./results/boolq_no_safety.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenBookQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5457, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_stem</th>\n",
       "      <th>choices</th>\n",
       "      <th>answerKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7-980</td>\n",
       "      <td>The sun is responsible for</td>\n",
       "      <td>{'text': ['puppies learning new tricks', 'chil...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7-584</td>\n",
       "      <td>When standing miles away from Mount Rushmore</td>\n",
       "      <td>{'text': ['the mountains seem very close', 'th...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                 question_stem  \\\n",
       "0  7-980                    The sun is responsible for   \n",
       "1  7-584  When standing miles away from Mount Rushmore   \n",
       "\n",
       "                                             choices answerKey  \n",
       "0  {'text': ['puppies learning new tricks', 'chil...         D  \n",
       "1  {'text': ['the mountains seem very close', 'th...         D  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenBookQA\n",
    "ob_qa = load_dataset(\"allenai/openbookqa\")\n",
    "obqa = pd.concat([ob_qa['train'].to_pandas(), ob_qa['validation'].to_pandas()])\n",
    "print(obqa.shape)\n",
    "obqa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "obqa['choices'] = obqa['choices'].apply(get_obqa_label)\n",
    "obqa['inference_prompt'] = obqa.apply(lambda x: get_prompt(x, 'obqa'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "obqa_sampling = SamplingParams(n=1, \n",
    "                               max_tokens=2,\n",
    "                               top_p=0.9,\n",
    "                               temperature=0.0,\n",
    "                               # frequency_penalty=1.0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0, len(obqa)-1)\n",
    "    output = model.generate(obqa['inference_prompt'].tolist()[index], obqa_sampling)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(output[0].outputs[0].text, obqa['answerKey'].tolist()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5457/5457 [02:05<00:00, 43.66it/s]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(obqa['inference_prompt'].tolist(), obqa_sampling)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "C         4654\n",
       "B          479\n",
       "D          256\n",
       "A           65\n",
       "Others       3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "for ele in output:\n",
    "    all_results.append(ele.outputs[0].text)\n",
    "obqa['generated_response'] = all_results\n",
    "obqa['predicted_label'] = obqa['generated_response'].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "obqa['predicted_label'] = obqa['predicted_label'].apply(preprocess_response_choices)\n",
    "obqa.predicted_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answerKey\n",
       "A    0.275609\n",
       "B    0.247389\n",
       "D    0.246839\n",
       "C    0.230163\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obqa['answerKey'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.071467839472238"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obqa[obqa['answerKey'] == obqa['predicted_label']].shape[0]*100.0/obqa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "obqa.to_csv(\"./results/obqa_no_safety.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17951, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goal</th>\n",
       "      <th>sol1</th>\n",
       "      <th>sol2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When boiling butter, when it's ready, you can</td>\n",
       "      <td>Pour it onto a plate</td>\n",
       "      <td>Pour it into a jar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To permanently attach metal legs to a chair, y...</td>\n",
       "      <td>Weld the metal together to get it to stay firm...</td>\n",
       "      <td>Nail the metal together to get it to stay firm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                goal  \\\n",
       "0      When boiling butter, when it's ready, you can   \n",
       "1  To permanently attach metal legs to a chair, y...   \n",
       "\n",
       "                                                sol1  \\\n",
       "0                               Pour it onto a plate   \n",
       "1  Weld the metal together to get it to stay firm...   \n",
       "\n",
       "                                                sol2  label  \n",
       "0                                 Pour it into a jar      1  \n",
       "1  Nail the metal together to get it to stay firm...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PIQA dataset\n",
    "piqa_data = load_dataset(\"piqa\")\n",
    "piqa = pd.concat([piqa_data['train'].to_pandas(), piqa_data['validation'].to_pandas(), piqa_data['test'].to_pandas()])\n",
    "piqa = piqa[piqa['label'] != -1]\n",
    "print(piqa.shape)\n",
    "piqa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    8988\n",
       "0    8963\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piqa.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "piqa['choices'] = piqa.apply(lambda x: f\"A) {x['sol1']}\\nB) {x['sol2']}\", 1)\n",
    "piqa['inference_prompt'] = piqa.apply(lambda x: get_prompt(x, 'piqa'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "piqa_sampling = SamplingParams(n=1, \n",
    "                               max_tokens=2,\n",
    "                               top_p=0.9,\n",
    "                               temperature=0.0,\n",
    "                               # frequency_penalty=1.0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0, len(piqa)-1)\n",
    "    output = model.generate(piqa['inference_prompt'].tolist()[index], piqa_sampling)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(output[0].outputs[0].text, piqa['label'].tolist()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  31%|███       | 5505/17951 [03:08<06:48, 30.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-03 21:44:14 scheduler.py:195] Input prompt (1341 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  74%|███████▎  | 13208/17951 [07:27<02:08, 36.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-03 21:48:33 scheduler.py:195] Input prompt (1322 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 17951/17951 [10:10<00:00, 29.40it/s]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(piqa['inference_prompt'].tolist(), piqa_sampling)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "1        10985\n",
       "0         6964\n",
       "Other        2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "for ele in output:\n",
    "    all_results.append(ele.outputs[0].text)\n",
    "piqa['generated_response'] = all_results\n",
    "piqa['predicted_label'] = piqa['generated_response'].apply(preprocess_piqa_response)\n",
    "piqa['predicted_label'] = piqa['predicted_label'].apply(str)\n",
    "piqa.predicted_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.500696\n",
       "0    0.499304\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piqa['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.38103726811877"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piqa[piqa['label'].apply(str) == piqa['predicted_label']].shape[0]*100.0/piqa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "piqa.to_csv(\"./results/piqa_no_safety.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpfulness Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained('Ray2333/gpt2-large-helpful-reward_model')\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/gpt2-large-helpful-reward_model',\n",
    "                num_labels=1, \n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=0,\n",
    "                )\n",
    "q, a = \"\\n\\nHuman: I just came out of from jail, any suggestion of my future? \\n\\nAssistant:\", \"Sorry, I don't understand.\"\n",
    "inputs = rm_tokenizer(q, a, return_tensors='pt', truncation=True)\n",
    "with torch.no_grad():\n",
    "  reward = reward_model(**(inputs.to(0))).logits[0].cpu().detach().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a7910fc18b495a94b51bb95b8fb68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c2f6134b8f44aba00e314365027171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load all helpfulness/harmless subsets (share the same schema)\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir='helpful-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nHuman: Do you know why turkeys became the ...</td>\n",
       "      <td>\\n\\nHuman: Do you know why turkeys became the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nHuman: How can I find out what types of bu...</td>\n",
       "      <td>\\n\\nHuman: How can I find out what types of bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nHuman: Can you tell me about wild dogs? No...</td>\n",
       "      <td>\\n\\nHuman: Can you tell me about wild dogs? No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nHuman: what do i do if i stepped in dog po...</td>\n",
       "      <td>\\n\\nHuman: what do i do if i stepped in dog po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nHuman: Search for best ac/dc songs.\\n\\nAss...</td>\n",
       "      <td>\\n\\nHuman: Search for best ac/dc songs.\\n\\nAss...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              chosen  \\\n",
       "0  \\n\\nHuman: Do you know why turkeys became the ...   \n",
       "1  \\n\\nHuman: How can I find out what types of bu...   \n",
       "2  \\n\\nHuman: Can you tell me about wild dogs? No...   \n",
       "3  \\n\\nHuman: what do i do if i stepped in dog po...   \n",
       "4  \\n\\nHuman: Search for best ac/dc songs.\\n\\nAss...   \n",
       "\n",
       "                                            rejected  \n",
       "0  \\n\\nHuman: Do you know why turkeys became the ...  \n",
       "1  \\n\\nHuman: How can I find out what types of bu...  \n",
       "2  \\n\\nHuman: Can you tell me about wild dogs? No...  \n",
       "3  \\n\\nHuman: what do i do if i stepped in dog po...  \n",
       "4  \\n\\nHuman: Search for best ac/dc songs.\\n\\nAss...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['test'].to_pandas()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        To be honest, I don’t know anything about tha...\n",
       "1        About 150 species of butterflies live in Oreg...\n",
       "2        Sure! Here’s some fun facts: A wolf is a larg...\n",
       "3        That’s reasonable, but even that is probably ...\n",
       "4        You probably already know a lot about AC/DC. ...\n",
       "                              ...                        \n",
       "2349     Here’s a list of a few of my favorite artists...\n",
       "2350     Oh yes, I meant, free in that people can get ...\n",
       "2351                                      Just follow me.\n",
       "2352     Well, first you’ll need to fill out the forms...\n",
       "2353     Why not give him a banana or an apple or a ha...\n",
       "Name: chosen, Length: 2354, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['chosen'].apply(lambda x: x.split(\"Assistant:\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
