{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this notebook, we will look at the average helpfulness reward across different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\"./data/\")\n",
    "all_files = []\n",
    "for ele in files:\n",
    "    if 'csv' in ele:\n",
    "        all_files.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2_large_0_safety.csv',\n",
       " 'gpt2_large_1000_safety.csv',\n",
       " 'gpt2_large_100_safety.csv',\n",
       " 'gpt2_large_1500_safety.csv',\n",
       " 'gpt2_large_2000_safety.csv',\n",
       " 'gpt2_large_300_safety.csv',\n",
       " 'gpt2_large_500_safety.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2_large_0_safety.csv\n",
      "0.837890625\n",
      "952\n",
      "\n",
      "\n",
      "\n",
      "gpt2_large_100_safety.csv\n",
      "0.884765625\n",
      "952\n",
      "\n",
      "\n",
      "\n",
      "gpt2_large_300_safety.csv\n",
      "0.88671875\n",
      "952\n",
      "\n",
      "\n",
      "\n",
      "gpt2_large_500_safety.csv\n",
      "0.859375\n",
      "952\n",
      "\n",
      "\n",
      "\n",
      "gpt2_large_1000_safety.csv\n",
      "0.8671875\n",
      "952\n",
      "\n",
      "\n",
      "\n",
      "gpt2_large_1500_safety.csv\n",
      "0.8984375\n",
      "952\n",
      "\n",
      "\n",
      "\n",
      "gpt2_large_2000_safety.csv\n",
      "0.94140625\n",
      "952\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for num in [0, 100, 300, 500, 1000, 1500, 2000]:\n",
    "    file = f\"gpt2_large_{num}_safety.csv\"\n",
    "    df = pd.read_csv(\"./data/\" + file)\n",
    "    print(file)\n",
    "    print(df['gpt2_large_reward'].describe()['50%'])\n",
    "    print(len(df))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-05-05 16:15:18.914824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-05 16:15:21.153338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# automated metrics\n",
    "import torch\n",
    "from sacrebleu import corpus_bleu\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import BERTScorer\n",
    "from bert_score import score as bert_score\n",
    "from bleurt import score\n",
    "from rouge_score import rouge_scorer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "# BART Score\n",
    "import sys\n",
    "sys.path.append(\"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/Evaluation_Metrics/MedBART/BARTScore\")\n",
    "# from bart_score import BARTScorer\n",
    "# dd = \"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/Evaluation_Metrics/MedBART/BARTScore/\"\n",
    "# # bart_score.pth\n",
    "# bart_scorer = BARTScorer(device='cuda:0', checkpoint=\"facebook/bart-large-cnn\")\n",
    "# bart_scorer.load(path= dd + 'bart_score.pth')\n",
    "\n",
    "bert_score = BERTScorer(model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\", rescale_with_baseline=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate:\n",
    "    def __init__(self):\n",
    "        self.scorer = None\n",
    "        \n",
    "class ScoreBleu(Evaluate):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scorer = sentence_bleu\n",
    "        \n",
    "    def score(self, ref, output):\n",
    "        score = self.scorer([ref.split()], output.split())\n",
    "        return score\n",
    "    \n",
    "class ScoreRouge(Evaluate):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "    def score(self, ref, output):\n",
    "        rouge_scores = self.scorer.score(ref, output)\n",
    "        rouge1 = rouge_scores['rouge1'].fmeasure\n",
    "        rouge2 = rouge_scores['rouge2'].fmeasure\n",
    "        rougeL = rouge_scores['rougeL'].fmeasure\n",
    "        return rouge1, rouge2, rougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_automated_metrics(input_data):\n",
    "    prompts = input_data['instructions'].tolist()\n",
    "    refs = input_data['golden_response'].to_list()\n",
    "    outputs = input_data['outputs'].to_list()\n",
    "\n",
    "    res = {}\n",
    "\n",
    "    e = ScoreBleu()\n",
    "    scores = [e.score(x,y) for x,y in zip(refs,outputs)]\n",
    "    bleu  = sum(scores)/len(scores)\n",
    "\n",
    "    res['bleu'] = bleu\n",
    "    print(\"BLUE Score : \", bleu)\n",
    "    # print(\"Bleu done\")\n",
    "    \n",
    "    e = ScoreRouge()\n",
    "    scores = [e.score(x,y) for x,y in zip(refs,outputs)]\n",
    "    r1 = [x[0] for x in scores]\n",
    "    r2 = [x[1] for x in scores]\n",
    "    rl = [x[2] for x in scores]\n",
    "\n",
    "    res['rouge-1'] = sum(r1)/len(r1)\n",
    "    res['rouge-2'] = sum(r2)/len(r2)\n",
    "    res['rouge-l'] = sum(rl)/len(rl)\n",
    "    \n",
    "    print(\"Rouge-1 : \", res['rouge-1'])\n",
    "    print(\"Rouge-2 : \", res['rouge-2'])\n",
    "    print(\"Rouge-L : \", res['rouge-l'])\n",
    "    \n",
    "    \n",
    "    p,r,f = bert_score.score(refs, outputs)\n",
    "    torch.cuda.empty_cache()\n",
    "    res['bert_score'] = f.mean().item()\n",
    "    print(\"BERT Score : \", f.mean().item())\n",
    "    # print(\"Bert score done\")\n",
    "    \n",
    "    return res\n",
    "\n",
    "    # # bart score\n",
    "    # bart = bart_scorer.score(refs, outputs, batch_size=4)\n",
    "    # torch.cuda.empty_cache()\n",
    "    # res['bart'] = sum(bart)/len(bart)\n",
    "    \n",
    "    # # save it\n",
    "    # # Open the file in write mode\n",
    "    # with open(self.output_dir + 'autometrics_eval.json', \"w\") as json_file:\n",
    "    #     # Write the data to the file\n",
    "    #     json.dump(res, json_file)\n",
    "    # print(\"Saved at \", self.output_dir + 'autometrics_eval.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/responses_alpaca_test_0_safe.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE Score :  0.03832360988694305\n",
      "Rouge-1 :  0.33417670897998064\n",
      "Rouge-2 :  0.12786351632409734\n",
      "Rouge-L :  0.22673123381204507\n",
      "BERT Score :  0.19510111212730408\n",
      "{'bleu': 0.03832360988694305, 'rouge-1': 0.33417670897998064, 'rouge-2': 0.12786351632409734, 'rouge-l': 0.22673123381204507, 'bert_score': 0.19510111212730408}\n",
      "\n",
      "\n",
      "\n",
      "./data/responses_alpaca_test_100_safe.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE Score :  0.03681847235814658\n",
      "Rouge-1 :  0.33399165110635654\n",
      "Rouge-2 :  0.1273945083881824\n",
      "Rouge-L :  0.22615412735870588\n",
      "BERT Score :  0.19357430934906006\n",
      "{'bleu': 0.03681847235814658, 'rouge-1': 0.33399165110635654, 'rouge-2': 0.1273945083881824, 'rouge-l': 0.22615412735870588, 'bert_score': 0.19357430934906006}\n",
      "\n",
      "\n",
      "\n",
      "./data/responses_alpaca_test_300_safe.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE Score :  0.03712921352207869\n",
      "Rouge-1 :  0.33579427936565515\n",
      "Rouge-2 :  0.1281203126892478\n",
      "Rouge-L :  0.2275912230315672\n",
      "BERT Score :  0.19800803065299988\n",
      "{'bleu': 0.03712921352207869, 'rouge-1': 0.33579427936565515, 'rouge-2': 0.1281203126892478, 'rouge-l': 0.2275912230315672, 'bert_score': 0.19800803065299988}\n",
      "\n",
      "\n",
      "\n",
      "./data/responses_alpaca_test_500_safe.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE Score :  0.03586079439225947\n",
      "Rouge-1 :  0.33164004682061704\n",
      "Rouge-2 :  0.12593505093426155\n",
      "Rouge-L :  0.2244506734314742\n",
      "BERT Score :  0.1927282214164734\n",
      "{'bleu': 0.03586079439225947, 'rouge-1': 0.33164004682061704, 'rouge-2': 0.12593505093426155, 'rouge-l': 0.2244506734314742, 'bert_score': 0.1927282214164734}\n",
      "\n",
      "\n",
      "\n",
      "./data/responses_alpaca_test_1000_safe.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE Score :  0.03691406493249383\n",
      "Rouge-1 :  0.3330483300848055\n",
      "Rouge-2 :  0.12758020789452273\n",
      "Rouge-L :  0.2236626425215876\n",
      "BERT Score :  0.19175806641578674\n",
      "{'bleu': 0.03691406493249383, 'rouge-1': 0.3330483300848055, 'rouge-2': 0.12758020789452273, 'rouge-l': 0.2236626425215876, 'bert_score': 0.19175806641578674}\n",
      "\n",
      "\n",
      "\n",
      "./data/responses_alpaca_test_1500_safe.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE Score :  0.0365300183379189\n",
      "Rouge-1 :  0.33405830482843824\n",
      "Rouge-2 :  0.12706444057292765\n",
      "Rouge-L :  0.2253591064876454\n",
      "BERT Score :  0.19825227558612823\n",
      "{'bleu': 0.0365300183379189, 'rouge-1': 0.33405830482843824, 'rouge-2': 0.12706444057292765, 'rouge-l': 0.2253591064876454, 'bert_score': 0.19825227558612823}\n",
      "\n",
      "\n",
      "\n",
      "./data/responses_alpaca_test_2000_safe.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUE Score :  0.0350018649537188\n",
      "Rouge-1 :  0.33204666644759817\n",
      "Rouge-2 :  0.1240970516903776\n",
      "Rouge-L :  0.22274105318560322\n",
      "BERT Score :  0.1923235058784485\n",
      "{'bleu': 0.0350018649537188, 'rouge-1': 0.33204666644759817, 'rouge-2': 0.1240970516903776, 'rouge-l': 0.22274105318560322, 'bert_score': 0.1923235058784485}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"./data/alpaca_test.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "all_metrics = []\n",
    "for num in [0, 100, 300, 500, 1000, 1500, 2000]:\n",
    "    file = f\"./data/responses_alpaca_test_{num}_safe.json\"\n",
    "    print(file)\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    del data['parameters']\n",
    "    data = pd.DataFrame(data)\n",
    "    data['golden_response'] = data['instructions'].map(dict(zip(test_data['instructions'], test_data['outputs']))) \n",
    "    curr_res = get_automated_metrics(data)\n",
    "    all_metrics.append(curr_res)\n",
    "    print(curr_res)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
