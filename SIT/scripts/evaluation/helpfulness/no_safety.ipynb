{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this notebook, we will take I-Alpaca dataset - promotes helpfulness (general instruction following ability). Compute bleu and other autometrics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### alpaca-cleaned dataset\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"yahma/alpaca-cleaned\")['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open(\"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/data/training/alpaca_small.json\") as f:\n",
    "    train = json.load(f)\n",
    "train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19358, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['instruction'].isin(data['instruction'])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19359, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['instruction'].isin(train['instruction'])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32401, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[~data['instruction'].isin(train['instruction'])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(952, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14947</th>\n",
       "      <td>If you are making a savory pie and want to add...</td>\n",
       "      <td>Suggest which pizza topping would go best with...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6237</th>\n",
       "      <td>Euler's Formula, often expressed as \"e^(ix)= c...</td>\n",
       "      <td>Write a one-sentence description of Euler's Fo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49853</th>\n",
       "      <td>The firing process, also referred to as termin...</td>\n",
       "      <td>Explain the firing process for a company that ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12365</th>\n",
       "      <td>An example of a chemical change that occurs ev...</td>\n",
       "      <td>Give an example of a chemical change that occu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22749</th>\n",
       "      <td>Petra is an ancient city located in Jordan, be...</td>\n",
       "      <td>Research the Lost City of Petra and summarize ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  output  \\\n",
       "14947  If you are making a savory pie and want to add...   \n",
       "6237   Euler's Formula, often expressed as \"e^(ix)= c...   \n",
       "49853  The firing process, also referred to as termin...   \n",
       "12365  An example of a chemical change that occurs ev...   \n",
       "22749  Petra is an ancient city located in Jordan, be...   \n",
       "\n",
       "                                             instruction input  \n",
       "14947  Suggest which pizza topping would go best with...        \n",
       "6237   Write a one-sentence description of Euler's Fo...        \n",
       "49853  Explain the firing process for a company that ...        \n",
       "12365  Give an example of a chemical change that occu...        \n",
       "22749  Research the Lost City of Petra and summarize ...        "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = data[~data['instruction'].isin(train['instruction'])].sample(n=1500, random_state=2024)\n",
    "test_data = test_data[test_data['input'] == \"\"]\n",
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./data/alpaca_test.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"instructions\": test_data['instruction'].tolist(),\n",
    "            \"outputs\": test_data['output'].tolist(),\n",
    "            \"inputs\" : test_data['input'].tolist() \n",
    "        },\n",
    "        f,\n",
    "        indent=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsaluru_umass_edu/.conda/envs/self_llm_env/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/scripts\")\n",
    "from prompter import Prompter\n",
    "\n",
    "import os\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\" # my huggingface key to access llama models\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from tap import Tap\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "# Check if MPS is available\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "except:  # noqa: E722\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "class GenerationArguments:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # self.base_model = \"yahma/llama-7b-hf\"\n",
    "        self.base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "        self.lora_weights = \"safep/lora-alpaca-small-100-yahma\" # this is after training\n",
    "        self.load_8bit = True\n",
    "\n",
    "        # generation arguments\n",
    "        self.max_new_tokens = 256\n",
    "        self.num_beams = 4\n",
    "        self.top_k = 40\n",
    "        self.top_p = 0.75\n",
    "        self.temperature = 0.1\n",
    "            \n",
    "\n",
    "        ## Input and output files\n",
    "        self.prompt_template_path = \"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/data/templates/alpaca.json\"\n",
    "        # self.input_path = \"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/data/safety_tuned/I-Alpaca.json\"\n",
    "        self.input_path = \"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/data/safety_tuned/I-MaliciousInstructions.json\"\n",
    "        self.output_path = \"sample.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompter,\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128,\n",
    "    stream_output=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = prompter.generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        do_sample=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    generate_params = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_scores\": True,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "    }\n",
    "\n",
    "    # Without streaming\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "    return prompter.get_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main function\n",
    "def main(args):\n",
    "    # Load the input data (.json)\n",
    "    input_path = args.input_path\n",
    "    with open(input_path) as f:\n",
    "        input_data = json.load(f)\n",
    "    instructions = input_data[\"instructions\"]\n",
    "    inputs = input_data[\"inputs\"]\n",
    "\n",
    "    # Validate the instructions and inputs\n",
    "    if instructions is None:\n",
    "        raise ValueError(\"No instructions provided\")\n",
    "    if inputs is None or len(inputs) == 0:\n",
    "        inputs = [None] * len(instructions)\n",
    "    elif len(instructions) != len(inputs):\n",
    "        raise ValueError(\n",
    "            f\"Number of instructions ({len(instructions)}) does not match number of inputs ({len(inputs)})\"\n",
    "        )\n",
    "\n",
    "    # Load the prompt template\n",
    "    prompter = Prompter(\"alpaca\")\n",
    "\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
    "    if device == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.base_model,\n",
    "            load_in_8bit=args.load_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        # model = PeftModel.from_pretrained(\n",
    "        #     model,\n",
    "        #     args.lora_weights,\n",
    "        #     torch_dtype=torch.float16,\n",
    "        # )\n",
    "    else:\n",
    "        raise ValueError(\"No GPU available - resubmit the jobs\")\n",
    "\n",
    "    if not args.load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    # Generate the outputs\n",
    "    outputs = []\n",
    "    for instruction, input in tqdm(\n",
    "        zip(instructions, inputs),\n",
    "        total=len(instructions),\n",
    "        desc=f\"Evaluate {args.lora_weights}\",\n",
    "    ):\n",
    "        output = evaluate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompter=prompter,\n",
    "            instruction=instruction,\n",
    "        )\n",
    "        outputs.append(output)\n",
    "        \n",
    "    # Save the outputs\n",
    "    # basename = os.path.basename(input_path)\n",
    "\n",
    "    output_path = \"sample_final.json\"\n",
    "    # # Check if the output path directory exists\n",
    "    # if not os.path.exists(os.path.dirname(output_path)):\n",
    "    #     os.makedirs(os.path.dirname(output_path))\n",
    "    # Save the outputs to the output path\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"parameters\": {\n",
    "                    \"model\": args.base_model,\n",
    "                    \"prompt_template\": args.prompt_template_path,\n",
    "                    \"lora_weights\": args.lora_weights,\n",
    "                    \"load_8bit\": args.load_8bit,\n",
    "                },\n",
    "                \"inputs\": inputs,\n",
    "                \"instructions\": instructions,\n",
    "                \"outputs\": outputs,\n",
    "            },\n",
    "            f,\n",
    "            indent=4,\n",
    "        )\n",
    "    return instructions, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = GenerationArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-CoNa.json',\n",
       " 'I-Alpaca.json',\n",
       " 'I-PhysicalSafetyUnsafe.json',\n",
       " 'I-PhysicalSafetySafe.json',\n",
       " 'I-MaliciousInstructions.json',\n",
       " 'I-Controversial.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir(\"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/data/safety_tuned\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea28fe34321e40b9b236943813133290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate safep/lora-alpaca-small-100-yahma:   0%|          | 0/100 [00:00<?, ?it/s]2024-05-04 18:52:43.587356: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-04 18:52:43.638171: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-04 18:52:50.114545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Evaluate safep/lora-alpaca-small-100-yahma: 100%|██████████| 100/100 [43:20<00:00, 26.00s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "default_args.input_path = \"./data/alpaca_test.json\"\n",
    "default_args.base_model = \"/project/pi_hongyu_umass_edu/zonghai/clinical-llm-alignment/durga_sandeep/self_rewarding/LLM_Alignment/safety_llama_paper/models/safety_llama_replicate/merged_model\"\n",
    "insts, inps, responses = main(default_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_path = f\"./data/responses_alpaca_test.json\"\n",
    "# #output_path = os.path.join(args.output_path, args.lora_weights, basename)\n",
    "# # Check if the output path directory exists\n",
    "# if not os.path.exists(os.path.dirname(output_path)):\n",
    "#     os.makedirs(os.path.dirname(output_path))\n",
    "# Save the outputs to the output path\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"parameters\": {\n",
    "                \"model\": default_args.base_model,\n",
    "                \"prompt_template\": default_args.prompt_template_path,\n",
    "                \"lora_weights\": default_args.lora_weights,\n",
    "                \"load_8bit\": default_args.load_8bit,\n",
    "            },\n",
    "            \"inputs\": inps,\n",
    "            \"instructions\": insts,\n",
    "            \"outputs\": responses,\n",
    "        },\n",
    "        f,\n",
    "        indent=4,\n",
    "    )\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Model\n",
    "- GPT2-Large-helpfuness-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained('Ray2333/gpt2-large-helpful-reward_model')\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/gpt2-large-helpful-reward_model',\n",
    "                num_labels=1, torch_dtype=torch.bfloat16,\n",
    "                device_map=0,\n",
    "                )\n",
    "q, a = \"\\n\\nHuman: I just came out of from jail, any suggestion of my future? \\n\\nAssistant:\", \"Sorry, I don't understand.\"\n",
    "inputs = rm_tokenizer(q, a, return_tensors='pt', truncation=True)\n",
    "with torch.no_grad():\n",
    "  reward = reward_model(**(inputs.to(0))).logits[0].cpu().detach().item()\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.09375"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, a = \"\\n\\nHuman: I just came out of from jail, any suggestion of my future? \\n\\nAssistant:\", \"Sure, you can always restart your career in right direction\"\n",
    "inputs = rm_tokenizer(q, a, return_tensors='pt', truncation=True)\n",
    "with torch.no_grad():\n",
    "  reward = reward_model(**(inputs.to(0))).logits[0].cpu().detach().item()\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.88671875"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./data/responses_alpaca_test.json\"\n",
    "with open(output_path) as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['inputs', 'instructions', 'outputs'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data[data['inputs'] == \"\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['inputs'] == \"\"]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "for i in range(len(data)):\n",
    "  \n",
    "  q, a = f\"\\n\\nHuman: {data.iloc[i]['instructions']} \\n\\nAssistant:\", data.iloc[i]['outputs']\n",
    "  inputs = rm_tokenizer(q, a, return_tensors='pt', truncation=True)\n",
    "  with torch.no_grad():\n",
    "    reward = reward_model(**(inputs.to(0))).logits[0].cpu().detach().item()\n",
    "    torch.cuda.empty_cache()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  all_rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gpt2_large_reward'] = all_rewards\n",
    "data.to_csv('./data/gpt2_large_no_safety.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt2_large_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.940952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.458707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.664062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.150391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.785156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.851562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.437500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gpt2_large_reward\n",
       "count          63.000000\n",
       "mean            0.940952\n",
       "std             1.458707\n",
       "min            -1.664062\n",
       "25%            -0.150391\n",
       "50%             0.785156\n",
       "75%             1.851562\n",
       "max             4.437500"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['gpt2_large_reward']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
